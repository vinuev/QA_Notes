{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8 \n",
    "# import urllib2\n",
    "# import sys, os\n",
    "# import re\n",
    "# import string\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import signal\n",
    "import time\n",
    "import sys\n",
    "import requesocks\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_request(arg=None):\n",
    "    \"\"\"Your http request.\"\"\"\n",
    "    time.sleep(2)\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timeout():\n",
    "    \"\"\"Timeout class using ALARM signal.\"\"\"\n",
    "    class Timeout(Exception):\n",
    "        pass\n",
    " \n",
    "    def __init__(self, sec):\n",
    "        self.sec = sec\n",
    " \n",
    "    def __enter__(self):\n",
    "        signal.signal(signal.SIGALRM, self.raise_timeout)\n",
    "        signal.alarm(self.sec)\n",
    " \n",
    "    def __exit__(self, *args):\n",
    "        signal.alarm(0)    # disable alarm\n",
    " \n",
    "    def raise_timeout(self, *args):\n",
    "        raise Timeout.Timeout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visible1(element):\n",
    "    if re.match('\\[.*\\]', str(element.encode('utf-8'))):\n",
    "         return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(result):\n",
    "    result1=''\n",
    "    for elem in result:\n",
    "        if elem.count(' ') <= 1:\n",
    "            continue\n",
    "        elem=elem.strip('\\n')\n",
    "        elem=elem.strip('\\t')\n",
    "        elem=elem.strip('\\r')\n",
    "        result1+=' '+elem\n",
    "    return result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Safari/605.1.15'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proxies = {\n",
    "    'http': 'http://127.0.0.1:1087',\n",
    "    'https': 'https://127.0.0.1:1087'}\n",
    "# 请求这样写\n",
    "# resp = requests.get(url, proxies=proxies, headers={\"User-Agent\": \"xxxx\"})\n",
    "# resp = requests.get(query, proxies=proxies, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top10_GOOGLE(q):\n",
    "    fp=open('./docs/link_wise_docs_'+q+'.json','w')\n",
    "    links=[]\n",
    "    line_count=0\n",
    "    docs=[]\n",
    "    start=1\n",
    "    print \"\\n\\nCrawling for ---> \",q\n",
    "    crawled=0\n",
    "    # qflag=0\n",
    "    while crawled<10 and start<=11:\n",
    "        qflag=0\n",
    "        query='https://www.googleapis.com/customsearch/v1?key=AIzaSyBVB31PBqr7OBjunIjq8iPNb5L75pzTxzg&cx=000764406663250367259:rtvytnbpauk&q='+q+'&start='+str(start)\n",
    "        try:\n",
    "            with Timeout(40):\n",
    "                headers={'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Safari/605.1.15'}\n",
    "                result = requests.get(query, proxies=proxies, headers=headers)\n",
    "#                 result = requests.get(query)\n",
    "                r=result.json()\n",
    "                qflag=1\n",
    "        except Timeout.Timeout:\n",
    "            print \"\\n\\nTimeout Google\\n\\n\"\n",
    "        except:\n",
    "            print \"\\n\\nCould not crawl google... \",start,q\n",
    "        #print \"done link\"\n",
    "        ct=0\n",
    "        flag=0\n",
    "        if qflag==1 and 'items' in r and r['items'][0]['kind']!='':\n",
    "            for doc in r['items']:\n",
    "                #print \"link  \",start+ct, doc['link']\n",
    "                flag=0\n",
    "                try:\n",
    "                    with Timeout(60):\n",
    "                        #html = urllib2.urlopen(doc['link']).read() #urllib.request.urlopen(link)\n",
    "                        opener = urllib2.build_opener(urllib2.ProxyHandler(proxies))\n",
    "                        urllib2.install_opener(opener)\n",
    "                        req = urllib2.Request(doc['link'], headers=headers)\n",
    "                        html = urllib2.urlopen(req)\n",
    "                        flag=1\n",
    "                        \n",
    "                except Timeout.Timeout:\n",
    "                    flag=0\n",
    "                    #print \"\\n\\nTimeout\"\n",
    "                except:\n",
    "                    flag=0\n",
    "                    #print \"\\n\\nCould not crawl ... \",start+ct,doc['link']\n",
    "                if flag==1:\n",
    "                    links.append(doc['link'])\n",
    "                    crawled+=1\n",
    "                    soup = BeautifulSoup(html,\"lxml\")\n",
    "                    data = soup.findAll(text=True)\n",
    "                    result = list(filter(visible, data))\n",
    "                    result = list(filter(visible1, result))\n",
    "                    #result = ''.join(map(delReturn, result)) # \n",
    "                    #result = result.strip() # \n",
    "                    #result = re.sub('\\n{2,}', '\\n\\n', result) # \n",
    "                    result = process(result)\n",
    "                    result = unicodedata.normalize(u'NFKD', result).encode('ascii', 'ignore').decode('utf8')\n",
    "                    #result = re.sub('\\n', '', result) # \n",
    "                    result = re.sub(\"\\s\", ' ', result)\n",
    "                    result = re.sub(r'\\'','', result)\n",
    "                    fp.write(json.dumps({'link':doc['link'],'doc':result1.encode('utf-8')}))\n",
    "                    fp.write('\\n')\n",
    "                    fp1=open('./docs/doc'+str(crawled)+'.txt','w')\n",
    "                    # fp1=open('./docs/doc'+str(crawled)+'.txt','w')\n",
    "                    fp1.write('<doc id=\"'+str(crawled)+'\"'+'url=\"'+doc['link']+'\"'+'title=\"'+doc['title']+'\">\\n')\n",
    "                    fp1.write(doc['title']+'\\n')\n",
    "                    fp1.write(result.encode('utf-8'))\n",
    "                    fp1.write('\\n\\n\\n\\n</doc>')\n",
    "                    fp1.close()\n",
    "                    if len(docs)<10:\n",
    "                        docs.append('./docs/doc'+str(crawled)+'.txt')\n",
    "                #print \"Done writing\"\n",
    "                ct+=1\n",
    "        start+=10\n",
    "        #print \"Start \",start,crawled\n",
    "    pickle.dump(links,open('./links/google_search_links_'+q,'w'))\n",
    "    fp.close()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question='footballer of African descent who played in the FIFA 2018 final and the Euro 2016 final'\n",
    "### Crawling top 10 documents from Google \n",
    "print \"\\n\\nNo document provided...Crawling top 10 docs from Google...\"\n",
    "docs=get_top10_GOOGLE(question)\n",
    "print \"\\n\\nCrawling documents done...\",len(docs),docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
