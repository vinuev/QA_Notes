{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "# graphframe\n",
    "sc.addPyFile('/usr/local/Cellar/apache-spark/2.4.1/libexec/jars/graphframes-0.7.0-spark2.4-s_2.11.jar')\n",
    "\n",
    "# modules\n",
    "import sys\n",
    "from graphframes import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# library \n",
    "sys.path.append(\"/Users/jeanxu/Documents/UniLU/0_MasterThesis/5_Notes/QA_Notes/1_Test/python/project/library\")\n",
    "import get_answer_types_from_questions as gtQType\n",
    "import get_all_subjects_predicates_from_questions as gtQSub\n",
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "verbose = 0\n",
    "nlp = StanfordCoreNLP(r'/Users/jeanxu/PycharmProjects/QUEST_TerminalTest/Code/stanford-corenlp-full-2018-10-05')\n",
    "prune = 5\n",
    "question = 'when is national day of England'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting potential answer type from question\n",
    "f1 = 'question_type.txt'\n",
    "gtQType.get_answer_type_main(question,f1,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting query terms from question.\n",
    "s1 = 'question_subject_predicate.txt'\n",
    "gtQSub.get_sub_pred_ques_main(question,s1,nlp)\n",
    "aux_list = set()\n",
    "for l in open('auxiliary_verb_list.txt'):\n",
    "    l = l.strip()\n",
    "    aux_list.add(l.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read entities of question\n",
    "s11 = open(s1, 'r')\n",
    "q_ent = set()\n",
    "type_qent = {}\n",
    "for line in s11:\n",
    "    line = line.strip()\n",
    "    line = line.split()\n",
    "    s = line[0]\n",
    "    for i in range(1, len(line) - 1):\n",
    "        s += ' ' + line[i]\n",
    "    q_ent.add(s.lower())\n",
    "    type_qent[s.lower()] = line[len(line) - 1]\n",
    "if verbose:\n",
    "    print \"Query terms ->\", len(q_ent), q_ent\n",
    "if len(q_ent) > prune:\n",
    "    if verbose:\n",
    "        print \"Pruning..\"\n",
    "    p = frozenset(q_ent)\n",
    "    for s in p:\n",
    "        if type_qent[s] == 'P':\n",
    "            q_ent.remove(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'england', 'is', 'national day'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'england': 'NE', 'is': 'P', 'national day': 'NE'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_qent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = frozenset(q_ent)\n",
    "for s in p:\n",
    "    if s in aux_list and type_qent[s] == 'P':\n",
    "        q_ent.remove(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'england', 'is', 'national day'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "File_verticesTextRDD = \"/Users/jeanxu/Documents/UniLU/0_MasterThesis/4_Spark/ReferenceFromPaul/Graph-Small/Vertices/part-*\"\n",
    "File_edgesTextRDD = \"/Users/jeanxu/Documents/UniLU/0_MasterThesis/4_Spark/ReferenceFromPaul/Graph-Small/Edges/part-*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### load vertices\n",
    "verticesText0 = spark.read.csv(File_verticesTextRDD, header='false', inferSchema='false', sep='\\t')\n",
    "\n",
    "verticesText1 = verticesText0.select(\"_c1\",\"_c0\",\"_c2\",\"_c3\",\"_c4\")\\\n",
    ".withColumnRenamed(\"_c0\", \"nodeType\").withColumnRenamed(\"_c1\", \"id\")\\\n",
    ".withColumnRenamed(\"_c2\", \"attr1\").withColumnRenamed(\"_c3\", \"attr2\")\\\n",
    ".withColumnRenamed(\"_c4\", \"attr3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+--------------------+-------------------+\n",
      "| id|nodeType|attr1|               attr2|              attr3|\n",
      "+---+--------+-----+--------------------+-------------------+\n",
      "|  0|Sentence|    1|Politics of Burun...|Politics of Burundi|\n",
      "|  1|Sentence|    2|Executive power i...|Politics of Burundi|\n",
      "|  2|Sentence|    3|Legislative power...|Politics of Burundi|\n",
      "|  3|Sentence|    4|The political lan...|Politics of Burundi|\n",
      "|  4|Sentence|    5|The current Presi...|Politics of Burundi|\n",
      "|  5|Sentence|    6|Nkurunziza was th...|Politics of Burundi|\n",
      "|  6|Sentence|    7|In November 1995,...|Politics of Burundi|\n",
      "|  7|Sentence|    8|In July 1996, for...|Politics of Burundi|\n",
      "|  8|Sentence|    9|He declared himse...|Politics of Burundi|\n",
      "|  9|Sentence|   10|Widespread condem...|Politics of Burundi|\n",
      "| 10|Sentence|   11|Buyoya agreed in ...|Politics of Burundi|\n",
      "| 11|Sentence|   12|Nonetheless, figh...|Politics of Burundi|\n",
      "| 12|Sentence|   13|In June 1998, Buy...|Politics of Burundi|\n",
      "| 13|Sentence|   14|After facilitator...|Politics of Burundi|\n",
      "| 14|Sentence|   15|Under Mandela the...|Politics of Burundi|\n",
      "| 15|Sentence|   16|In April 2015 the...|Politics of Burundi|\n",
      "| 16|Sentence|   17|Protests in the c...|Politics of Burundi|\n",
      "| 17|Sentence|   18|The president is ...|Politics of Burundi|\n",
      "| 18|Sentence|   19|He nominates two ...|Politics of Burundi|\n",
      "| 19|Sentence|   20|The National Asse...|Politics of Burundi|\n",
      "+---+--------+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verticesText1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### load edges\n",
    "edgesText0 = spark.read.csv(File_edgesTextRDD)\n",
    "edgesText0 = edgesText0.select(f.translate(f.col(\"_c0\"), \"Edge(\", \"\").alias(\"src\"), \"_c1\", f.translate(f.col(\"_c2\"), \")\", \"\").alias(\"label\"))\n",
    "edgesText1 = edgesText0.select(\"*\").withColumnRenamed(\"_c1\", \"dst\")\n",
    "verticesText1J = verticesText1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+--------------------+-------------------+\n",
      "| id|nodeType|attr1|               attr2|              attr3|\n",
      "+---+--------+-----+--------------------+-------------------+\n",
      "|  0|Sentence|    1|Politics of Burun...|Politics of Burundi|\n",
      "|  1|Sentence|    2|Executive power i...|Politics of Burundi|\n",
      "|  2|Sentence|    3|Legislative power...|Politics of Burundi|\n",
      "|  3|Sentence|    4|The political lan...|Politics of Burundi|\n",
      "|  4|Sentence|    5|The current Presi...|Politics of Burundi|\n",
      "|  5|Sentence|    6|Nkurunziza was th...|Politics of Burundi|\n",
      "|  6|Sentence|    7|In November 1995,...|Politics of Burundi|\n",
      "|  7|Sentence|    8|In July 1996, for...|Politics of Burundi|\n",
      "|  8|Sentence|    9|He declared himse...|Politics of Burundi|\n",
      "|  9|Sentence|   10|Widespread condem...|Politics of Burundi|\n",
      "| 10|Sentence|   11|Buyoya agreed in ...|Politics of Burundi|\n",
      "| 11|Sentence|   12|Nonetheless, figh...|Politics of Burundi|\n",
      "| 12|Sentence|   13|In June 1998, Buy...|Politics of Burundi|\n",
      "| 13|Sentence|   14|After facilitator...|Politics of Burundi|\n",
      "| 14|Sentence|   15|Under Mandela the...|Politics of Burundi|\n",
      "| 15|Sentence|   16|In April 2015 the...|Politics of Burundi|\n",
      "| 16|Sentence|   17|Protests in the c...|Politics of Burundi|\n",
      "| 17|Sentence|   18|The president is ...|Politics of Burundi|\n",
      "| 18|Sentence|   19|He nominates two ...|Politics of Burundi|\n",
      "| 19|Sentence|   20|The National Asse...|Politics of Burundi|\n",
      "+---+--------+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verticesText1J.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgesText2 = edgesText1.join(verticesText1.select(\"id\",\"nodeType\"), edgesText1.src == verticesText1.select(\"id\",\"nodeType\").id, \"inner\")\n",
    "edgesText2 = edgesText2.withColumnRenamed(\"id\", \"src_id\").withColumnRenamed(\"nodeType\", \"src_nodeType\")\n",
    "edgesText3 = edgesText2.join(verticesText1.select(\"id\",\"nodeType\"), edgesText2.dst == verticesText1.select(\"id\",\"nodeType\").id, \"inner\")\n",
    "edgesText3 = edgesText3.withColumnRenamed(\"id\", \"dst_id\").withColumnRenamed(\"nodeType\", \"dst_nodeType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+-------+------------+-------+------------+\n",
      "|    src|    dst|              label| src_id|src_nodeType| dst_id|dst_nodeType|\n",
      "+-------+-------+-------------------+-------+------------+-------+------------+\n",
      "|1234113| 100010|contains the clause|1234113|    Sentence| 100010|      Clause|\n",
      "|3875588|1000240|contains the clause|3875588|    Sentence|1000240|      Clause|\n",
      "|3650624|1000280|contains the clause|3650624|    Sentence|1000280|      Clause|\n",
      "|2068926|1000665|contains the clause|2068926|    Sentence|1000665|      Clause|\n",
      "| 467648|1000795|contains the clause| 467648|    Sentence|1000795|      Clause|\n",
      "| 233944|1000839|contains the clause| 233944|    Sentence|1000839|      Clause|\n",
      "|4022932|1000888|contains the clause|4022932|    Sentence|1000888|      Clause|\n",
      "|1212133| 100140|contains the clause|1212133|    Sentence| 100140|      Clause|\n",
      "| 452611|1001866|contains the clause| 452611|    Sentence|1001866|      Clause|\n",
      "|1487892|1002011|contains the clause|1487892|    Sentence|1002011|      Clause|\n",
      "|1525764|1002185|contains the clause|1525764|    Sentence|1002185|      Clause|\n",
      "|1455756| 100227|contains the clause|1455756|    Sentence| 100227|      Clause|\n",
      "|2791683|1002442|contains the clause|2791683|    Sentence|1002442|      Clause|\n",
      "|2753656| 100263|contains the clause|2753656|    Sentence| 100263|      Clause|\n",
      "|3182725|1002783|contains the clause|3182725|    Sentence|1002783|      Clause|\n",
      "|1212312|1002883|contains the clause|1212312|    Sentence|1002883|      Clause|\n",
      "|1212752|1002887|contains the clause|1212752|    Sentence|1002887|      Clause|\n",
      "|3082848| 100320|contains the clause|3082848|    Sentence| 100320|      Clause|\n",
      "|3036946|1003202|contains the clause|3036946|    Sentence|1003202|      Clause|\n",
      "|3486666|1003366|contains the clause|3486666|    Sentence|1003366|      Clause|\n",
      "+-------+-------+-------------------+-------+------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edgesText3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### documnet vertice & edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### documnets---attributes\n",
    "doc_verticesText = verticesText1.filter(\"nodeType = 'Document'\")\n",
    "doc_verticesText = doc_verticesText.withColumn(\"did\", doc_verticesText.id).withColumn(\"dtitle\", doc_verticesText.attr1).drop(\"attr1\").drop(\"attr2\").drop(\"attr3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+--------------------+\n",
      "|  id|nodeType| did|              dtitle|\n",
      "+----+--------+----+--------------------+\n",
      "|4526|Document|4526|     Africa Alphabet|\n",
      "|4527|Document|4527|               Brain|\n",
      "|4528|Document|4528|     Andreas Aagesen|\n",
      "|4529|Document|4529|Australian Broadc...|\n",
      "|4530|Document|4530|The Elephant 6 Re...|\n",
      "|4531|Document|4531|Albert of Branden...|\n",
      "|4532|Document|4532|Telecommunication...|\n",
      "|4533|Document|4533|                 DMA|\n",
      "|4534|Document|4534|         Alford plea|\n",
      "|4535|Document|4535|        Claude Piron|\n",
      "|4536|Document|4536|Full disclosure (...|\n",
      "|4537|Document|4537|    Anthem of Europe|\n",
      "|4538|Document|4538|             Cytosol|\n",
      "|4539|Document|4539|            Barbagia|\n",
      "|4540|Document|4540|             Calcium|\n",
      "|4541|Document|4541|   Chomsky hierarchy|\n",
      "|4542|Document|4542| Critical philosophy|\n",
      "|4543|Document|4543|   Cultural movement|\n",
      "|4544|Document|4544|          Braveheart|\n",
      "|4545|Document|4545|                Bill|\n",
      "+----+--------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_verticesText.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentence---attributes\n",
    "# sen_verticesText = verticesText1.filter(\"nodeType = 'Sentence'\")\n",
    "sen_edges = edgesText3.filter(\"label = 'contains the sentence'\")\n",
    "sen_verticesTextJ_test = verticesText1.filter(\"nodeType = 'Sentence'\")\n",
    "sen_edgesJ = sen_edges.select(\"src_id\",\"src_nodeType\",\"dst_id\",\"dst_nodeType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+------------+\n",
      "|src_id|src_nodeType| dst_id|dst_nodeType|\n",
      "+------+------------+-------+------------+\n",
      "|626404|    Document|1014942|    Sentence|\n",
      "|626407|    Document|1015263|    Sentence|\n",
      "|626411|    Document|1015551|    Sentence|\n",
      "|626411|    Document|1015552|    Sentence|\n",
      "|626411|    Document|1015610|    Sentence|\n",
      "|626413|    Document|1015827|    Sentence|\n",
      "|626414|    Document|1015968|    Sentence|\n",
      "|626417|    Document|1016299|    Sentence|\n",
      "|626417|    Document|1016518|    Sentence|\n",
      "|626417|    Document|1016816|    Sentence|\n",
      "|626418|    Document|1017045|    Sentence|\n",
      "|626418|    Document|1017074|    Sentence|\n",
      "|626419|    Document|1017462|    Sentence|\n",
      "|626421|    Document|1017551|    Sentence|\n",
      "|626422|    Document|1017716|    Sentence|\n",
      "|626422|    Document|1017805|    Sentence|\n",
      "|626422|    Document|1017849|    Sentence|\n",
      "|626422|    Document|1018090|    Sentence|\n",
      "|626422|    Document|1018100|    Sentence|\n",
      "|626392|    Document|1018247|    Sentence|\n",
      "+------+------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_edgesJ.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conbine src_id\n",
    "sen_edgesJ_test1 = sen_edgesJ.join(doc_verticesText, sen_edgesJ.src_id==doc_verticesText.id,\"inner\")\n",
    "sen_edgesJ_test1 = sen_edgesJ_test1.drop(\"id\").drop(\"nodeType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+------------+-------+--------------+\n",
      "| src_id|src_nodeType| dst_id|dst_nodeType|    did|        dtitle|\n",
      "+-------+------------+-------+------------+-------+--------------+\n",
      "|1019881|    Document|3245237|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245224|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245236|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245232|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245222|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245228|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245238|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245239|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245223|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245229|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245231|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245225|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245235|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245230|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245227|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245234|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245221|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245233|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245226|    Sentence|1019881|Emperor Ninken|\n",
      "|1019881|    Document|3245220|    Sentence|1019881|Emperor Ninken|\n",
      "+-------+------------+-------+------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_edgesJ_test1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conbine dst_id\n",
    "sen_verticesTextJ_test1 = sen_edgesJ_test1.join(sen_verticesTextJ_test,sen_edgesJ_test1.dst_id==sen_verticesTextJ_test.id,\"inner\")\n",
    "sen_verticesTextJ_test2 = sen_verticesTextJ_test1.drop(\"attr2\").drop(\"attr3\")\n",
    "sen_verticesText = sen_verticesTextJ_test2.drop(\"src_id\").drop(\"src_nodeType\").drop(\"dst_id\").drop(\"dst_nodeType\")\n",
    "sen_verticesText = sen_verticesText.withColumnRenamed(\"attr1\",\"sid\")\n",
    "sen_verticesText = sen_verticesText.select(\"id\",\"nodeType\",\"did\",\"dtitle\",\"sid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------------+---+\n",
      "|     id|nodeType|   did|              dtitle|sid|\n",
      "+-------+--------+------+--------------------+---+\n",
      "|1014942|Sentence|626404|              Genome|102|\n",
      "|1015263|Sentence|626407|             Alcohol| 99|\n",
      "|1015551|Sentence|626411|             Cartoon|  5|\n",
      "|1015552|Sentence|626411|             Cartoon|  6|\n",
      "|1015610|Sentence|626411|             Cartoon| 64|\n",
      "|1015827|Sentence|626413|Charlton Athletic...|190|\n",
      "|1015968|Sentence|626414|          Battleship|135|\n",
      "|1016299|Sentence|626417|          Bangladesh| 93|\n",
      "|1016518|Sentence|626417|          Bangladesh|312|\n",
      "|1016816|Sentence|626417|          Bangladesh|610|\n",
      "|1017045|Sentence|626418|               Ghost| 40|\n",
      "|1017074|Sentence|626418|               Ghost| 69|\n",
      "|1017462|Sentence|626419|United States For...|133|\n",
      "|1017551|Sentence|626421|         Amoxicillin| 38|\n",
      "|1017716|Sentence|626422|         El Salvador|112|\n",
      "|1017805|Sentence|626422|         El Salvador|201|\n",
      "|1017849|Sentence|626422|         El Salvador|245|\n",
      "|1018090|Sentence|626422|         El Salvador|486|\n",
      "|1018100|Sentence|626422|         El Salvador|496|\n",
      "|1018247|Sentence|626392|Brouwer fixed-poi...| 16|\n",
      "+-------+--------+------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_verticesText.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clause---attributes\n",
    "clause_verticesText_test = verticesText1.filter(\"nodeType = 'Clause'\")\n",
    "clause_edges = edgesText3.filter(\"label = 'contains the clause'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dst_id\n",
    "clause_verticesTextJ_test = clause_verticesText_test.join(clause_edges.select(\"src_id\",\"src_nodeType\",\"dst_id\",\"dst_nodeType\"), clause_verticesText_test.id==clause_edges.dst_id, \"inner\")\n",
    "clause_verticesTextJ_test = clause_verticesTextJ_test.drop(\"src_nodeType\").drop(\"dst_id\").drop(\"dst_nodeType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+-----+--------------------+-------+\n",
      "|     id|nodeType|               attr1|attr2|               attr3| src_id|\n",
      "+-------+--------+--------------------+-----+--------------------+-------+\n",
      "| 100010|  Clause|(\"Kerensky\", \"ord...|  SVO|            Kerensky|1234113|\n",
      "|1000240|  Clause|(\"to the paternal...|  SVC|to the paternal h...|3875588|\n",
      "|1000280|  Clause|(\"roughly speakin...|  SVO|roughly speaking ...|3650624|\n",
      "|1000665|  Clause|(\"sympatric speci...|  SVC|sympatric speciation|2068926|\n",
      "|1000795|  Clause|(\"The growth of t...|  SVO|The growth of the...| 467648|\n",
      "|1000839|  Clause|(\"the priests\", \"...|  SVA|         the priests| 233944|\n",
      "|1000888|  Clause|(\"the absolute va...|  SVC|  the absolute value|4022932|\n",
      "| 100140|  Clause|(\"A memorial plaq...|  SVA|   A memorial plaque|1212133|\n",
      "|1001866|  Clause|(\"1QDan\", \"is\", \"...|  SVC|               1QDan| 452611|\n",
      "|1002011|  Clause|(\"the use of Afri...|  SVC|the use of Africa...|1487892|\n",
      "|1002185|  Clause|(\"he\", \"won\", \"th...|  SVO|                  he|1525764|\n",
      "| 100227|  Clause|(\"The following y...|  SVC|  The following year|1455756|\n",
      "|1002442|  Clause|(\"Beetles\", \"are\"...|  SVC|             Beetles|2791683|\n",
      "| 100263|  Clause|(\"it\", \"changes\",...|  SVO|                  it|2753656|\n",
      "|1002783|  Clause|(\"Disraeli\", \"hop...|  SVA|            Disraeli|3182725|\n",
      "|1002883|  Clause|(\"they\", \"reject\"...|  SVO|                they|1212312|\n",
      "|1002887|  Clause|(\"The result\", \"w...|  SVC|          The result|1212752|\n",
      "| 100320|  Clause|(\"chicks\", \"fledg...|  SVO|              chicks|3082848|\n",
      "|1003202|  Clause|(\"of stable indiv...|  SVC|of stable indivis...|3036946|\n",
      "|1003366|  Clause|(\"they\", \"emphasi...|  SVO|                they|3486666|\n",
      "+-------+--------+--------------------+-----+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clause_verticesTextJ_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_verticesTextJ=sen_verticesText.withColumnRenamed(\"id\",\"senid\")##important, need to keep different column name with others when \"JOIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine src_id\n",
    "clause_verticesText_test1 = clause_verticesTextJ_test.join(sen_verticesTextJ.select(\"senid\",\"did\",\"dtitle\",\"sid\"), clause_verticesTextJ_test.src_id==sen_verticesTextJ.senid, \"inner\")\n",
    "clause_verticesText_test2 = clause_verticesText_test1.drop(\"attr2\").drop(\"attr3\").drop(\"src_id\").drop(\"sen_id\")\n",
    "clause_verticesText = clause_verticesText_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+-------+------+--------------------+---+\n",
      "|     id|nodeType|               attr1|  senid|   did|              dtitle|sid|\n",
      "+-------+--------+--------------------+-------+------+--------------------+---+\n",
      "|2911087|  Clause|(\"Both the proces...|1014942|626404|              Genome|102|\n",
      "|1732278|  Clause|(\"alcohols\", \"are...|1015263|626407|             Alcohol| 99|\n",
      "|3236586|  Clause|(\"cartoon\", \"came...|1015551|626411|             Cartoon|  5|\n",
      "|3493546|  Clause|(\"to animated fil...|1015552|626411|             Cartoon|  6|\n",
      "|3493545|  Clause|(\"it\", \"began\", \"...|1015552|626411|             Cartoon|  6|\n",
      "|3327830|  Clause|(\"Thomson\", \"capi...|1015610|626411|             Cartoon| 64|\n",
      "|1230382|  Clause|(\"The ground-shar...|1015827|626413|Charlton Athletic...|190|\n",
      "|2658479|  Clause|(\"the Austro-Hung...|1015968|626414|          Battleship|135|\n",
      "|2658478|  Clause|(\"The Adriatic\", ...|1015968|626414|          Battleship|135|\n",
      "| 276441|  Clause|(\"British rule\", ...|1016299|626417|          Bangladesh| 93|\n",
      "| 276440|  Clause|(\"Several rebelli...|1016299|626417|          Bangladesh| 93|\n",
      "| 464234|  Clause|(\"The Bangladesh ...|1016518|626417|          Bangladesh|312|\n",
      "| 427430|  Clause|(\"Ivory and brass...|1016816|626417|          Bangladesh|610|\n",
      "| 386149|  Clause| (\"the one\", \"died\")|1017045|626418|               Ghost| 40|\n",
      "| 386148|  Clause|(\"the ghost or sp...|1017045|626418|               Ghost| 40|\n",
      "| 386147|  Clause|(\"Some people\", \"...|1017045|626418|               Ghost| 40|\n",
      "|3457439|  Clause|(\"Supernatural ac...|1017074|626418|               Ghost| 69|\n",
      "|1880973|  Clause|(\"the Act\", \"woul...|1017462|626419|United States For...|133|\n",
      "|1880971|  Clause|(\"Rulings for the...|1017462|626419|United States For...|133|\n",
      "|1880972|  Clause|(\"the government\"...|1017462|626419|United States For...|133|\n",
      "+-------+--------+--------------------+-------+------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clause_verticesText.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change clause to predicate\n",
    "clause_verticesText1 = clause_verticesText.withColumn(\"attr1\", f.split(clause_verticesText['attr1'], \",\"))\n",
    "predicate_verticesText = clause_verticesText.withColumn(\"predicate\", f.split(\"attr1\", \",\")[1])\n",
    "predicate_verticesText1 = predicate_verticesText.select(\"*\", f.translate(f.col(\"predicate\"), \"[\\\"()]\", \"\").alias(\"predicate1\"))\n",
    "predicate_verticesText2 = predicate_verticesText1.withColumn(\"nodeType1\",f.lit(\"Predicate\"))\n",
    "predicate_verticesText = predicate_verticesText2.select(\"id\",\"nodeType1\",\"did\",\"dtitle\",\"sid\",\"predicate1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_verticesText=predicate_verticesText.withColumnRenamed(\"nodeType1\",\"nodeType\").withColumnRenamed(\"predicate1\",\"predicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+---+-----------------+\n",
      "|     id|nodeType1|   did|              dtitle|sid|       predicate1|\n",
      "+-------+---------+------+--------------------+---+-----------------+\n",
      "|2911087|Predicate|626404|              Genome|102|       can result|\n",
      "|1732278|Predicate|626407|             Alcohol| 99|              are|\n",
      "|3236586|Predicate|626411|             Cartoon|  5|             came|\n",
      "|3493546|Predicate|626411|             Cartoon|  6|        resembled|\n",
      "|3493545|Predicate|626411|             Cartoon|  6|            began|\n",
      "|3327830|Predicate|626411|             Cartoon| 64|      capitalized|\n",
      "|1230382|Predicate|626413|Charlton Athletic...|190|              was|\n",
      "|2658479|Predicate|626414|          Battleship|135|         remained|\n",
      "|2658478|Predicate|626414|          Battleship|135|              was|\n",
      "| 276441|Predicate|626417|          Bangladesh| 93|        displaced|\n",
      "| 276440|Predicate|626417|          Bangladesh| 93|        broke out|\n",
      "| 464234|Predicate|626417|          Bangladesh|312|      was enacted|\n",
      "| 427430|Predicate|626417|          Bangladesh|610|        were used|\n",
      "| 386149|Predicate|626418|               Ghost| 40|             died|\n",
      "| 386148|Predicate|626418|               Ghost| 40|     never leaves|\n",
      "| 386147|Predicate|626418|               Ghost| 40|          believe|\n",
      "|3457439|Predicate|626418|               Ghost| 69|          is said|\n",
      "|1880973|Predicate|626419|United States For...|133| would be amended|\n",
      "|1880971|Predicate|626419|United States For...|133|         prompted|\n",
      "|1880972|Predicate|626419|United States For...|133|  had overreached|\n",
      "+-------+---------+------+--------------------+---+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicate_verticesText.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mention---mention edges\n",
    "entity_edges = edgesText3.filter(\"label = 'is disambiguated as'\")\n",
    "NewMentionEdges = entity_edges.groupBy(\"dst_id\").agg(f.collect_list('src_id').alias('NewMentionEdges'))\n",
    "NewMentionEdges = NewMentionEdges.withColumnRenamed(\"NewMentionEdges\",\"Mention\")\n",
    "NewMentionEdges = NewMentionEdges.select(\"Mention\")\n",
    "NewMentionEdges = NewMentionEdges.withColumn(\"Mention1\",NewMentionEdges.Mention)\n",
    "NewMentionEdgesJ = NewMentionEdges.withColumn(\"Mention1\", f.explode(\"Mention1\"))\n",
    "\n",
    "NewMentionEdgesJ1 = NewMentionEdgesJ.withColumn(\"Mention\", f.explode(\"Mention\"))\n",
    "NewMentionEdgesJ2 = NewMentionEdgesJ1.filter(NewMentionEdgesJ1.Mention!=NewMentionEdgesJ1.Mention1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|Mention|Mention1|\n",
      "+-------+--------+\n",
      "|4417364| 4417352|\n",
      "|4417356| 4417352|\n",
      "|7915519| 4417352|\n",
      "|7171576| 4417352|\n",
      "|7171574| 4417352|\n",
      "|4417352| 4417364|\n",
      "|4417356| 4417364|\n",
      "|7915519| 4417364|\n",
      "|7171576| 4417364|\n",
      "|7171574| 4417364|\n",
      "|4417352| 4417356|\n",
      "|4417364| 4417356|\n",
      "|7915519| 4417356|\n",
      "|7171576| 4417356|\n",
      "|7171574| 4417356|\n",
      "|4417352| 7915519|\n",
      "|4417364| 7915519|\n",
      "|4417356| 7915519|\n",
      "|7171576| 7915519|\n",
      "|7171574| 7915519|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NewMentionEdgesJ2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##add type node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_verticesText_test=verticesText1.filter(\"nodeType = 'Sentence'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_Type_verticesText_test=sen_verticesText_test.select(\"attr2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               attr2|\n",
      "+--------------------+\n",
      "|Politics of Burun...|\n",
      "|Executive power i...|\n",
      "|Legislative power...|\n",
      "|The political lan...|\n",
      "|The current Presi...|\n",
      "|Nkurunziza was th...|\n",
      "|In November 1995,...|\n",
      "|In July 1996, for...|\n",
      "|He declared himse...|\n",
      "|Widespread condem...|\n",
      "|Buyoya agreed in ...|\n",
      "|Nonetheless, figh...|\n",
      "|In June 1998, Buy...|\n",
      "|After facilitator...|\n",
      "|Under Mandela the...|\n",
      "|In April 2015 the...|\n",
      "|Protests in the c...|\n",
      "|The president is ...|\n",
      "|He nominates two ...|\n",
      "|The National Asse...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_Type_verticesText_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_symbols(s):\n",
    "\ts=s.replace('(',' ')\n",
    "\ts=s.replace(')',' ')\n",
    "\ts=s.replace('[',' ')\n",
    "\ts=s.replace(']',' ')\n",
    "\ts=s.replace('{',' ')\n",
    "\ts=s.replace('}',' ')\n",
    "\ts=s.replace('|',' ')\n",
    "\ts=s.replace('\"',' ')\n",
    "\ts=s.strip()\n",
    "\treturn s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaceS_udf=udf(replace_symbols,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_Type_verticesText_test1 = sen_Type_verticesText_test.select(\"attr2\",replaceS_udf(sen_Type_verticesText_test[\"attr2\"]).alias(\"RS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_Type_verticesText_test1 = sen_Type_verticesText_test.withColumn(\"RPS\",replaceS_udf(sen_Type_verticesText_test[\"attr2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen_Type_verticesText_test1 =sen_Type_verticesText_test.select(gtQSub.replace_symbols(\"attr2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               attr2|                 RPS|\n",
      "+--------------------+--------------------+\n",
      "|Politics of Burun...|Politics of Burun...|\n",
      "|Executive power i...|Executive power i...|\n",
      "|Legislative power...|Legislative power...|\n",
      "|The political lan...|The political lan...|\n",
      "|The current Presi...|The current Presi...|\n",
      "|Nkurunziza was th...|Nkurunziza was th...|\n",
      "|In November 1995,...|In November 1995,...|\n",
      "|In July 1996, for...|In July 1996, for...|\n",
      "|He declared himse...|He declared himse...|\n",
      "|Widespread condem...|Widespread condem...|\n",
      "|Buyoya agreed in ...|Buyoya agreed in ...|\n",
      "|Nonetheless, figh...|Nonetheless, figh...|\n",
      "|In June 1998, Buy...|In June 1998, Buy...|\n",
      "|After facilitator...|After facilitator...|\n",
      "|Under Mandela the...|Under Mandela the...|\n",
      "|In April 2015 the...|In April 2015 the...|\n",
      "|Protests in the c...|Protests in the c...|\n",
      "|The president is ...|The president is ...|\n",
      "|He nominates two ...|He nominates two ...|\n",
      "|The National Asse...|The National Asse...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_Type_verticesText_test1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_Type_verticesText_test1=sen_Type_verticesText_test1.drop(\"attr2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.addPyFile('/Users/jeanxu/Documents/UniLU/0_MasterThesis/5_Notes/QA_Notes/1_Test/python/project/library/hearstPatterns.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hearstPatterns as hP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = hP.HearstPatterns(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test find_hyponyms\n",
    "# h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hearstPatterns import HearstPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h=HearstPatterns(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test find_hyponyms\n",
    "# x=h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test use pandas\n",
    "# sen_Type_verticesText_test2[\"RPS\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen_Type_verticesText_test2[\"RPS\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h.find_hyponyms(sen_Type_verticesText_test2[\"RPS\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost a lot of time give up\n",
    "# sen_Type_verticesText_test2=sen_Type_verticesText_test1.toPandas()\n",
    "# HP = set()\n",
    "# for i in range(sen_Type_verticesText_test2[\"RPS\"].count()):\n",
    "#     HP1 = set(h.find_hyponyms(sen_Type_verticesText_test2[\"RPS\"][i]))#\n",
    "#     HP = HP.union(HP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, extended = False):\n",
    "        self.__chunk_patterns = r\"\"\" #  helps us find noun phrase chunks\n",
    "                NP: {<DT|PP\\$>?<JJ>*<NN>+}\n",
    "                    {<NNP>+}\n",
    "                    {<NNS>+}\n",
    "        \"\"\"\n",
    "\n",
    "        self.__np_chunker = nltk.RegexpParser(self.__chunk_patterns) # create a chunk parser \n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern, the firsr Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "                (\"(NP_\\w+ (, )?such as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(such NP_\\w+ (, )?as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?including (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?especially (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "            ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?any other NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?some other NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?is a NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?was a NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?were a NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?are a NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?like (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"such (NP_\\w+ (, )?as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?like other NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?one of the NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?one of these NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?one of those NP_\\w+)\", \"last\"),\n",
    "                (\"examples of (NP_\\w+ (, )?is (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"examples of (NP_\\w+ (, )?are (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?are examples of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?is example of NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?for example (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?wich is called NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?which is named NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?mainly (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?mostly (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?notably (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?particularly (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?principally (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?in particular (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?except (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?other than (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?e.g. (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?i.e. (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?a kind of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?kinds of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?form of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?forms of NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?which looks like NP_\\w+)\", \"last\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?which sounds like NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?which are similar to (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?which is similar to (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?examples of this is (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?examples of this are (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?types (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )? NP_\\w+ types)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?whether (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(compare (NP_\\w+ ?(, )?)+(and |or )?with NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )?compared to (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"(NP_\\w+ (, )?among them (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?as NP_\\w+)\", \"last\"),\n",
    "                (\"(NP_\\w+ (, )? (NP_\\w+ ? (, )?(and |or )?)+ for instance)\", \"first\"),\n",
    "                (\"((NP_\\w+ ?(, )?)+(and |or )?sort of NP_\\w+)\", \"last\"),\n",
    "            ])\n",
    "\n",
    "        self.__pos_tagger = PerceptronTagger()\n",
    "        \n",
    "    def prepare(self, rawtext):\n",
    "        sentences = nltk.sent_tokenize(rawtext.strip()) # NLTK default sentence segmenter\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences] # NLTK word tokenizer\n",
    "        sentences = [self.__pos_tagger.tag(sent) for sent in sentences] # NLTK POS tagger\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        sentences = self.prepare(rawtext.strip())\n",
    "\n",
    "        all_chunks = []\n",
    "        for sentence in sentences:\n",
    "            chunks = self.__np_chunker.parse(sentence) # parse the example sentence\n",
    "            #for chunk in chunks:\n",
    "            #   print(str(chunk))\n",
    "            all_chunks.append(self.prepare_chunks(chunks))\n",
    "        return all_chunks\n",
    "\n",
    "    def prepare_chunks(self, chunks):\n",
    "        # basically, if the chunk is NP, keep it as a string taht starts w/ NP and replace \" \" with _\n",
    "        # otherwise, keep the word.\n",
    "        # remove punct\n",
    "        # this is all done to make it super easy to apply the Hearst patterns...\n",
    "\n",
    "        terms = []\n",
    "        for chunk in chunks:\n",
    "            label = None\n",
    "            try: # gross hack to see if the chunk is simply a word or a NP, as we want. But non-NP fail on this method call\n",
    "                label = chunk.label()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if label is None: #means one word...\n",
    "                token = chunk[0]\n",
    "                pos = chunk[1]\n",
    "                if pos in ['.', ':', '-', '_']:\n",
    "                    continue\n",
    "                terms.append(token)\n",
    "            else:\n",
    "                np = \"NP_\"+\"_\".join([a[0] for a in chunk]) #This makes it easy to apply the Hearst patterns later\n",
    "                terms.append(np)\n",
    "        return ' '.join(terms)\n",
    "\n",
    "    def replace_np_sequences(self, sentence):\n",
    "        words = \"\"\n",
    "        first_word_in_sequence = False\n",
    "        for word in nltk.word_tokenize(sentence.replace(\"NP_\", \"_\")):\n",
    "            if word[0] == \"_\":\n",
    "                if not first_word_in_sequence:\n",
    "                    word = \"NP\" + word\n",
    "                    first_word_in_sequence = True\n",
    "                    words = words + \" \" + word\n",
    "                else:\n",
    "                    words += word\n",
    "            else:\n",
    "                words = words + \" \" + word\n",
    "                first_word_in_sequence = False\n",
    "        return words.strip()\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "        #print \"NP tagged-->\",np_tagged_sentences\n",
    "\n",
    "        for raw_sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged into a single NP, it's a chunk error\n",
    "\n",
    "            # find any N consecutive NP_ and merge them into one...\n",
    "            # So, something like: \"NP_foo NP_bar blah blah\" becomes \"NP_foo_bar blah blah\"\n",
    "            sentence = self.replace_np_sequences(raw_sentence)\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "\n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "                        #print(str(general))\n",
    "                        #print(str(nps))\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        #print(\"%s, %s\" % (specifics[i], general))\n",
    "                        hs = self.clean_hyponym_term(specifics[i])+\"-\"+self.clean_hyponym_term(general)\n",
    "                        hyponyms.append(hs)\n",
    "                        # hyponyms.append((self.clean_hyponym_term(general),self.clean_hyponym_term(general)))\n",
    "                        # return hyponyms\n",
    "        if hyponyms!=[]:\n",
    "            return hyponyms\n",
    "\n",
    "    # from pyspark.sql.functions import udf\n",
    "    # from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "    # find_hyponyms_udf = udf(lambda x: find_hyponyms(x), ArrayType(StringType()))\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\",\"\").replace(\"_\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=HearstPatterns(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red eye-symptoms',\n",
       " 'ocular pain-symptoms',\n",
       " 'visual acuity-symptoms',\n",
       " 'photophobia-symptoms']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.find_hyponyms(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_hyponyms_udf = udf(H.find_hyponyms,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_Type_verticesText_test2 = sen_Type_verticesText_test1.withColumn(\"Tp\",find_hyponyms_udf(sen_Type_verticesText_test1[\"RPS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 RPS|                  Tp|\n",
      "+--------------------+--------------------+\n",
      "|Politics of Burun...|                null|\n",
      "|Executive power i...|                null|\n",
      "|Legislative power...|                null|\n",
      "|The political lan...|                null|\n",
      "|The current Presi...|                null|\n",
      "|Nkurunziza was th...|                null|\n",
      "|In November 1995,...|                null|\n",
      "|In July 1996, for...|                null|\n",
      "|He declared himse...|                null|\n",
      "|Widespread condem...|                null|\n",
      "|Buyoya agreed in ...|                null|\n",
      "|Nonetheless, figh...|                null|\n",
      "|In June 1998, Buy...|                null|\n",
      "|After facilitator...|[Nelson Mandela-F...|\n",
      "|Under Mandela the...|                null|\n",
      "|In April 2015 the...|                null|\n",
      "|Protests in the c...|                null|\n",
      "|The president is ...|                null|\n",
      "|He nominates two ...|                null|\n",
      "|The National Asse...|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_Type_verticesText_test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "HType=sen_Type_verticesText_test2.select(\"Tp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "HType1=HType.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  Tp|\n",
      "+--------------------+\n",
      "|[Nelson Mandela-F...|\n",
      "|[ADTs-the abstrac...|\n",
      "|   [data-data types]|\n",
      "|[relations-constr...|\n",
      "|[lattices-structu...|\n",
      "|[S-an explicit in...|\n",
      "|    [ADT-parameters]|\n",
      "|[the structure-a ...|\n",
      "|[the old state-an...|\n",
      "|[the same input s...|\n",
      "|[ADT instance use...|\n",
      "|[ADT operations-m...|\n",
      "|  [interfaces-types]|\n",
      "|[C-an imperative ...|\n",
      "|[Awk-many scripti...|\n",
      "|[Alpha engineerin...|\n",
      "|[Vinod Dham-engin...|\n",
      "|  [Athlons-a result]|\n",
      "|[core voltages-Th...|\n",
      "|[applications-an ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HType1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "HType2=HType1.withColumn(\"Tp1\", f.explode(\"Tp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  Tp|                 Tp1|\n",
      "+--------------------+--------------------+\n",
      "|[Nelson Mandela-F...|Nelson Mandela-Fa...|\n",
      "|[ADTs-the abstrac...|ADTs-the abstraction|\n",
      "|   [data-data types]|     data-data types|\n",
      "|[relations-constr...|relations-constra...|\n",
      "|[lattices-structu...| lattices-structures|\n",
      "|[lattices-structu...|   groups-structures|\n",
      "|[S-an explicit in...|S-an explicit ins...|\n",
      "|    [ADT-parameters]|      ADT-parameters|\n",
      "|[the structure-a ...|the structure-a s...|\n",
      "|[the old state-an...|the old state-an ...|\n",
      "|[the same input s...|the same input st...|\n",
      "|[ADT instance use...|ADT instance uses...|\n",
      "|[ADT operations-m...|ADT operations-me...|\n",
      "|  [interfaces-types]|    interfaces-types|\n",
      "|[C-an imperative ...|C-an imperative l...|\n",
      "|[Awk-many scripti...|Awk-many scriptin...|\n",
      "|[Awk-many scripti...|Lua-many scriptin...|\n",
      "|[Awk-many scripti...|Perl-many scripti...|\n",
      "|[Alpha engineerin...|Alpha engineering...|\n",
      "|[Vinod Dham-engin...|Vinod Dham-engineers|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HType2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = HType2.withColumn(\"MentionName\", f.split(\"Tp1\", \"-\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "HType3 = df_temp.withColumn(\"Type\", f.split(\"Tp1\", \"-\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "HType3=HType3.drop(\"Tp\").drop(\"Tp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         MentionName|                Type|\n",
      "+--------------------+--------------------+\n",
      "|      Nelson Mandela|         Facilitator|\n",
      "|                ADTs|     the abstraction|\n",
      "|                data|          data types|\n",
      "|           relations|         constraints|\n",
      "|            lattices|          structures|\n",
      "|              groups|          structures|\n",
      "|                   S|an explicit insta...|\n",
      "|                 ADT|          parameters|\n",
      "|       the structure|   a separate entity|\n",
      "|       the old state|         an argument|\n",
      "|the same input st...|           arguments|\n",
      "|   ADT instance uses|          a function|\n",
      "|      ADT operations|             methods|\n",
      "|          interfaces|               types|\n",
      "|                   C|an imperative lan...|\n",
      "|                 Awk|many scripting la...|\n",
      "|                 Lua|many scripting la...|\n",
      "|                Perl|many scripting la...|\n",
      "|Alpha engineering...|              Compaq|\n",
      "|          Vinod Dham|           engineers|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HType3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HType=HType.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_verticesText_1=verticesText1.filter(\"nodeType = 'Mention'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_verticesText_1=men_verticesText_1.drop(\"attr3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+------------+\n",
      "|   id|nodeType|               attr1|       attr2|\n",
      "+-----+--------+--------------------+------------+\n",
      "|24975| Mention|       Freedom House|        MISC|\n",
      "|24976| Mention|                 NGO|ORGANIZATION|\n",
      "|24977| Mention|              Obiang|      PERSON|\n",
      "|24978| Mention|       Freedom House|        MISC|\n",
      "|24979| Mention|                  US|    LOCATION|\n",
      "|24980| Mention|       Freedom House|        MISC|\n",
      "|24981| Mention|                Root|      PERSON|\n",
      "|24982| Mention|           Queen Ask|        MISC|\n",
      "|24983| Mention|     Act of Congress|        MISC|\n",
      "|24984| Mention|          Public Law|        MISC|\n",
      "|24985| Mention|              Darwin|      PERSON|\n",
      "|24986| Mention|Evidence as to Ma...|        MISC|\n",
      "|24987| Mention|       Thomas Huxley|      PERSON|\n",
      "|24988| Mention|             Spanish|    LOCATION|\n",
      "|24989| Mention|       Mariano Rajoy|      PERSON|\n",
      "|24990| Mention|            European|    LOCATION|\n",
      "|24991| Mention|        Dragon 32/64|        MISC|\n",
      "|24992| Mention|       Dragon's Claw|        MISC|\n",
      "|24993| Mention|       Dragon's Claw|        MISC|\n",
      "|24994| Mention|                  UK|    LOCATION|\n",
      "+-----+--------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "men_verticesText_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_edges_1=edgesText3.filter(\"label = 'contains the mention'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_edges_1=mention_edges_1.drop(\"src\").drop(\"dst\").drop(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+------------+\n",
      "| src_id|src_nodeType| dst_id|dst_nodeType|\n",
      "+-------+------------+-------+------------+\n",
      "|2073024|      Clause|1007462|     Mention|\n",
      "|2849688|      Clause|1007636|     Mention|\n",
      "|2568628|      Clause|1007678|     Mention|\n",
      "| 117777|      Clause|1008797|     Mention|\n",
      "|1759866|      Clause|1009080|     Mention|\n",
      "|2846388|      Clause|1009129|     Mention|\n",
      "| 564274|      Clause|1009177|     Mention|\n",
      "| 935474|      Clause|1009407|     Mention|\n",
      "|1878077|      Clause|1009511|     Mention|\n",
      "|3643422|      Clause|1009562|     Mention|\n",
      "|1860471|      Clause|1010103|     Mention|\n",
      "|1860472|      Clause|1010108|     Mention|\n",
      "|2076128|      Clause|1010262|     Mention|\n",
      "| 588258|      Clause|1010351|     Mention|\n",
      "|2658225|      Clause|1010503|     Mention|\n",
      "| 321989|      Clause|1010766|     Mention|\n",
      "| 713827|      Clause|1010896|     Mention|\n",
      "|3699391|      Clause|1011171|     Mention|\n",
      "|3699391|      Clause|1011172|     Mention|\n",
      "|1240648|      Clause|1011187|     Mention|\n",
      "+-------+------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mention_edges_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dst_id\n",
    "mention_verticesText2 = mention_edges_1.join(men_verticesText_1, mention_edges_1.dst_id==men_verticesText_1.id, \"inner\")\n",
    "# clause_verticesTextJ_test = clause_verticesTextJ_test.drop(\"src_nodeType\").drop(\"dst_id\").drop(\"dst_nodeType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_verticesText2=mention_verticesText2.drop(\"id\").drop(\"nodeType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+------------+--------------------+--------+\n",
      "| src_id|src_nodeType| dst_id|dst_nodeType|               attr1|   attr2|\n",
      "+-------+------------+-------+------------+--------------------+--------+\n",
      "|2073024|      Clause|1007462|     Mention|     Newington Youth|LOCATION|\n",
      "|2849688|      Clause|1007636|     Mention|American Film Ins...|    MISC|\n",
      "|2568628|      Clause|1007678|     Mention|               Italy|LOCATION|\n",
      "| 117777|      Clause|1008797|     Mention|              Church|    MISC|\n",
      "|1759866|      Clause|1009080|     Mention|             Laurier|  PERSON|\n",
      "|2846388|      Clause|1009129|     Mention|Anders Behring Br...|  PERSON|\n",
      "| 564274|      Clause|1009177|     Mention|     White Stockings|    MISC|\n",
      "| 935474|      Clause|1009407|     Mention|             Eleanor|  PERSON|\n",
      "|1878077|      Clause|1009511|     Mention|  Bristol University|LOCATION|\n",
      "|3643422|      Clause|1009562|     Mention|               Earth|LOCATION|\n",
      "|1860471|      Clause|1010103|     Mention|                Bell|  PERSON|\n",
      "|1860472|      Clause|1010108|     Mention|                Bell|  PERSON|\n",
      "|2076128|      Clause|1010262|     Mention|             Western|LOCATION|\n",
      "| 588258|      Clause|1010351|     Mention|AFC Croydon Athletic|    MISC|\n",
      "|2658225|      Clause|1010503|     Mention|             Islands|LOCATION|\n",
      "| 321989|      Clause|1010766|     Mention|                 BVC|    MISC|\n",
      "| 713827|      Clause|1010896|     Mention|             Bermuda|LOCATION|\n",
      "|3699391|      Clause|1011171|     Mention|              Indian|LOCATION|\n",
      "|3699391|      Clause|1011172|     Mention|               India|LOCATION|\n",
      "|1240648|      Clause|1011187|     Mention|               Jabal|    MISC|\n",
      "+-------+------------+-------+------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mention_verticesText2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine src_id\n",
    "mention_verticesText3 = mention_verticesText2.join(predicate_verticesText, mention_verticesText2.src_id==predicate_verticesText.id, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_verticesText3=mention_verticesText3.drop(\"src_id\").drop(\"src_nodeType\").drop(\"id\").drop(\"nodeType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_verticesText3=mention_verticesText3.drop(\"predicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_verticesText3=mention_verticesText3.withColumnRenamed(\"dst_id\",\"id\").withColumnRenamed(\"dst_nodeType\",\"nodeType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+--------+-------+-------------------+---+\n",
      "|     id|nodeType|               attr1|   attr2|    did|             dtitle|sid|\n",
      "+-------+--------+--------------------+--------+-------+-------------------+---+\n",
      "|2216143| Mention|            Kerensky|  PERSON| 606008| Alexander Kerensky| 34|\n",
      "|2216144| Mention| February Revolution|    MISC| 606008| Alexander Kerensky| 34|\n",
      "|3579976| Mention|           Cathedral|    MISC| 561719|  Claude Auchinleck|101|\n",
      "|3579975| Mention|             St Paul|LOCATION| 561719|  Claude Auchinleck|101|\n",
      "|2086736| Mention|             African|LOCATION|2444643|       Black people|183|\n",
      "|2086735| Mention|                  US|LOCATION|2444643|       Black people|183|\n",
      "|3571984| Mention|Nordic Chess Cham...|    MISC|2420916|   Aron Nimzowitsch| 13|\n",
      "|3571985| Mention|          Copenhagen|LOCATION|2420916|   Aron Nimzowitsch| 13|\n",
      "|1679175| Mention|      Ottoman Sultan|  PERSON| 851331|Christopher Báthory| 27|\n",
      "|1679176| Mention|        Transylvania|LOCATION| 851331|Christopher Báthory| 27|\n",
      "|1997886| Mention|             Beetles|  PERSON| 269398|             Beetle|307|\n",
      "|4039815| Mention|            Disraeli|  PERSON|2756547|  Benjamin Disraeli|159|\n",
      "|  77509| Mention|                John|  PERSON|3107509|        Christology| 63|\n",
      "|  77511| Mention|              Christ|  PERSON|3107509|        Christology| 63|\n",
      "|  77510| Mention|                John|  PERSON|3107509|        Christology| 63|\n",
      "|3958645| Mention|          Strasbourg|LOCATION|3323686|   Council of Trent| 58|\n",
      "|3958647| Mention|         Württemberg|  PERSON|3323686|   Council of Trent| 58|\n",
      "|3958646| Mention|         Protestants|    MISC|3323686|   Council of Trent| 58|\n",
      "|3958650| Mention|    Charles de Guise|  PERSON|3323686|   Council of Trent| 58|\n",
      "|3958649| Mention|          The French|LOCATION|3323686|   Council of Trent| 58|\n",
      "+-------+--------+--------------------+--------+-------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mention_verticesText3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "HType4=HType3.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "##join type by heart\n",
    "mention_verticesText4=mention_verticesText3.join(HType3, mention_verticesText3.attr1==HType3.MentionName,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_verticesText5=mention_verticesText3.join(HType4, mention_verticesText3.attr1==HType4.MentionName,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+--------+-------+-----------------+---+-----------+-------------------+\n",
      "|     id|nodeType|               attr1|   attr2|    did|           dtitle|sid|MentionName|               Type|\n",
      "+-------+--------+--------------------+--------+-------+-----------------+---+-----------+-------------------+\n",
      "|3559179| Mention|            Artesian|LOCATION|1439190|     Ellis Island| 39|       null|               null|\n",
      "|2021479| Mention|              Baloch|  PERSON|1511954|      Afghanistan|388|       null|               null|\n",
      "|1884303| Mention|      Blackford Hill|  PERSON|2081882|        Edinburgh| 98|       null|               null|\n",
      "|2242617| Mention|      Caesar Cardini|  PERSON|1235788|     Caesar salad| 16|       null|               null|\n",
      "|2809399| Mention|Combatant Status ...|    MISC|3625401|     Abu Zubaydah|176|       null|               null|\n",
      "|4039815| Mention|            Disraeli|  PERSON|2756547|Benjamin Disraeli|159|   Disraeli|           Disraeli|\n",
      "|4039815| Mention|            Disraeli|  PERSON|2756547|Benjamin Disraeli|159|   Disraeli|Conservative leader|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|            seasons|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|          festivals|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|       celebrations|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|        festivities|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter| Christian holidays|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|             feasts|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|           holidays|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|          festivals|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter| Christian holidays|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|          festivals|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|         Holy Week |\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter| Christian holidays|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|               days|\n",
      "+-------+--------+--------------------+--------+-------+-----------------+---+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mention_verticesText4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+--------+-------+-----------------+---+-----------+--------------------+\n",
      "|     id|nodeType|               attr1|   attr2|    did|           dtitle|sid|MentionName|                Type|\n",
      "+-------+--------+--------------------+--------+-------+-----------------+---+-----------+--------------------+\n",
      "|3559179| Mention|            Artesian|LOCATION|1439190|     Ellis Island| 39|       null|                null|\n",
      "|2021479| Mention|              Baloch|  PERSON|1511954|      Afghanistan|388|       null|                null|\n",
      "|1884303| Mention|      Blackford Hill|  PERSON|2081882|        Edinburgh| 98|       null|                null|\n",
      "|2242617| Mention|      Caesar Cardini|  PERSON|1235788|     Caesar salad| 16|       null|                null|\n",
      "|2809399| Mention|Combatant Status ...|    MISC|3625401|     Abu Zubaydah|176|       null|                null|\n",
      "|4039815| Mention|            Disraeli|  PERSON|2756547|Benjamin Disraeli|159|   Disraeli| Conservative leader|\n",
      "|4039815| Mention|            Disraeli|  PERSON|2756547|Benjamin Disraeli|159|   Disraeli|            Disraeli|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|           festivals|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|         festivities|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|          Holy Week |\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|        celebrations|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|       Easter Monday|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|  Christian holidays|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|              feasts|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|             seasons|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|            holidays|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|        Christianity|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|any religious sig...|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|            the year|\n",
      "|2916387| Mention|              Easter|    MISC|3038896|           Easter|138|     Easter|                days|\n",
      "+-------+--------+--------------------+--------+-------+-----------------+---+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mention_verticesText5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###alignment 'Predicate' / 'Type' / 'mention' alignment edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicate alignment\n",
    "#predicate=auxlite(is, was...) & predicate !=auxlist\n",
    "#1-get all predicat with id in column\n",
    "#2-delete all auxlist(was, is, are)\n",
    "#3-find same predicate in column (with different id)\n",
    "#4-get predicate glove\n",
    "#5-get weight and generate edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(world, gdict):\n",
    "    veclen = 300\n",
    "    g_pred = {}\n",
    "    g_ent = {}\n",
    "    g_type = {}\n",
    "    g_ques = {}\n",
    "    done = set()\n",
    "\n",
    "    for n in G.nodes():\n",
    "        if n not in done:\n",
    "            done.add(n)\n",
    "            nn = n.split(':')\n",
    "            nw1 = nn[0].replace('-', ' ').split()\n",
    "            avec = np.zeros(veclen)\n",
    "            c = 0.0\n",
    "            for el in nw1:\n",
    "                if el in gdict and el.lower() not in stop_list:\n",
    "                    # if option=='GLOVE':\n",
    "                    #\tavec=np.add(avec,gdict[el])\n",
    "                    # else:\n",
    "                    avec = np.add(avec, np.array(gdict[el]))\n",
    "                    c += 1.0\n",
    "            if c > 0:\n",
    "                avec = np.divide(avec, c)\n",
    "\n",
    "                if nn[1] == 'Predicate':\n",
    "                    g_pred[n] = avec.tolist()\n",
    "                else:\n",
    "                    if nn[1] == 'Entity':\n",
    "                        g_ent[n] = avec.tolist()\n",
    "                    else:\n",
    "                        g_type[n] = avec.tolist()\n",
    "        # if verbose:\n",
    "        # print len(G.nodes()),len(done)\n",
    "\n",
    "    for n in q_ent:\n",
    "        if n not in done:\n",
    "            done.add(n)\n",
    "            nw1 = n.replace('-', ' ').split()\n",
    "            avec = np.zeros(veclen)\n",
    "            c = 0.0\n",
    "            for el in nw1:\n",
    "                if el in gdict and el.lower() not in stop_list:\n",
    "                    # if option=='GLOVE':\n",
    "                    #\tavec=np.add(avec,gdict[el])\n",
    "                    # else:\n",
    "                    avec = np.add(avec, np.array(gdict[el]))\n",
    "                    c += 1.0\n",
    "            if c > 0:\n",
    "                avec = np.divide(avec, c)\n",
    "                g_ques[n] = avec.tolist()\n",
    "\n",
    "    return g_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.13 64-bit ('QUEST': conda)",
   "language": "python",
   "name": "python271364bitquestconda39a3e5c16cfd48579e21bcb7b60777eb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
